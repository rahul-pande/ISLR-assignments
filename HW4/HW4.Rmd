---
title: "DS502- HW3"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1. (10 points) Section 6.8, page 259, question 2

(a) iii. holds true. Lasso puts a budget constraint on the parameters which decreases the model variance and it reduces overfitting. However, when we put a constraint, the model bias increases. From the bias-variance trade-off concept we can say that the lasso regression will give better prediction when its increase in bias is less that its decrease in variance.

(b) iii. holds true. Like Lasso above, Ridge also puts a budget constraint on the parameters which decreases the model variance and it reduces overfitting. However, when we put a constraint, the model bias increases. Similarly as above, from the bias-variance trade-off concept we can say that the Ridge regression will give better prediction when its increase in bias is less that its decrease in variance.

(c) ii. holds true. Since non-linear methods are more flexible, they have higher variance than least squares regression but lower bias. Again, from the bias variance trade-off, if the increase on model variance is less than the decrease in bias, then non-linear model will have better prediction accuracy.

### 2. (20 points) Section 6.8, page264, question 11

(a)
```{r 6 11 a i, echo=TRUE}
library(MASS)
library(leaps)
library(glmnet)
attach(Boston)

colSums(sapply(Boston, is.na))
# No NAs in the dataset

# k-fold cross validation
k = 10

n = dim(Boston)[1]
p = dim(Boston)[2]-1

set.seed(123)
folds = sample(rep(1:k, length = nrow(Boston)), replace = T)

form.subset = as.formula("crim ~ .")

cv.errors = matrix(NA, k, p)

for (i in 1:k) {
  subset.fit <- regsubsets(form.subset, Boston[folds!=i, ], nvmax = p)
  for (n.subset in 1:p) {
    m.mat <- model.matrix(form.subset, Boston[folds==i, ])
    best.coef <- coef(subset.fit, id=n.subset)
    pred <- m.mat[, names(best.coef)] %*% best.coef
    # mean squared error
    error = mean((Boston[folds==i, ]$crim - pred)^2)
    cv.errors[i,n.subset] = error
  }
  
}
# root mean squared values
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, type = "b")

which(rmse.cv == min(rmse.cv))
rmse.cv[which(rmse.cv == min(rmse.cv))]
```

```{r 6 11 a ii, echo=TRUE}
# Lasso regression

X = model.matrix(crim ~ ., data = Boston)[, -1]
y = Boston$crim
cv.lasso = cv.glmnet(X, y, type.measure = "mse", nfolds = 10)
plot(cv.lasso)

coef(cv.lasso)[,1]

# One standard error lamda to avoid overfitting
chosen.lambda = cv.lasso$lambda.1se

# root mean square error for the chosen lamdba
sqrt(cv.lasso$cvm[cv.lasso$lambda == chosen.lambda])

```


```{r 6 11 a iii, echo=TRUE}

cv.ridge = cv.glmnet(X, y, type.measure = "mse", alpha = 0, nfolds = 10)
plot(cv.ridge)

coef(cv.ridge)

# One standard error lamda to avoid overfitting
chosen.lambda = cv.ridge$lambda.1se

# root mean square error for the chosen lamdba
sqrt(cv.ridge$cvm[cv.ridge$lambda == chosen.lambda])

```

```{r 6 11 a iv, echo=TRUE}
library(pls)

pcr.fit = pcr(crim ~ ., data = Boston, scale = TRUE, validation = "CV", segments= 10)
summary(pcr.fit)

# variance explanation
plot(pcr.fit$Xvar/sum(pcr.fit$Xvar))

# validation plot
validationplot(pcr.fit,val.type='MSEP')

ncomp = 4

set.seed(123)
folds = sample(rep(1:k, length = nrow(Boston)), replace = T)

cv.errors = rep(0, k)

for (i in 1:k) {
  pcr.fit <- pcr(crim ~ ., data = Boston[folds !=i, ], scale = TRUE)
  pred = predict( pcr.fit, Boston[folds==i, ], ncomp=ncomp)
  error = mean((Boston[folds==i, ]$crim - pred)^2)
  cv.errors[i] = error
}
sqrt(mean(cv.errors))

```


(b)
```{r 6 11 b, echo=TRUE}
results <- rbind(
  c("Best Subset", 6.633116, 9),
  c("Lasso Regression", 7.549995, 1),
  c("Ridge Regression", 7.359946, 13),
  c("PCR", 6.875975, 4)
)
colnames(results) <- c("Method", "MSE", "# predictors")
knitr::kable(results)
```

From the above table, we chose PCR model with 4 predictors as it has the mse very close to the lowest mse and is simpler model than the best subset model, since it has 4 predictors against the 9 predictors and thus has lower chances of overfitting the data. Second choice would be Best Subset model with 9 predictors as it has the lowest cross validation mse and has less number of predictors than Ridge Regression. Lasso Regression here seems to be underfit in this case.

(c) No. PCR has only 4 predictors since from the graph, after 4 predictors, adding another predictor does not increase the explained variance a lot. Hence we have taken only 4 predictors. We can also have criterion like taking n components which explain at least x% of the variance.

### 3. (10 points) Section 7.9, Page 298, question 3

We have $b_{1}(X)=X , b_{2}(X)=(X-1)^{2} * I(X \ge 1)$

For $X \ge 1, I(X \ge 1) = 1$, and $X < 1, I(X \ge 1) = 0$

Substituting $\hat\beta_0 = 1, \hat\beta_1 = 1, \hat\beta_2 = âˆ’2$ in

$Y =\beta_0 +\beta_1 b_1(X)+ \beta_2b_2(X) + \epsilon$

We get,

$\hat Y =1 + b_1(X) -2 b_2(X)$

For $X \ge 1, \hat Y =1 + X - 2(X - 1)^{2} = -1 + 5X - 2X^2$, and for $X < 1, \hat Y = 1 + X$

```{r 6.8 2, echo=TRUE}
x_lower = seq(-2, 1, by = 0.05)
x_upper = seq(1, 2, by = 0.05)

y_lower = 1 + x_lower
y_upper = -1 + 5 * x_upper - 2 * (x_upper ^ 2)

x <- c(x_lower, x_upper)
y <- c(y_lower, y_upper)

plot(x,y)
```

$Y = 1 + X, for X<1$
$Now, for X=0, Y = 1$

Therefore y-intercept is 1. Slope is 1 when $X<1$ and by taking derivative for $Y$ where $X\ge1$, we get slope as $5 - 4 X$

### 4. (10 points) Section 7.9, Page 298, question 4

Similary from above,
we can split the function into multiple domains. Since there are a lot of cuts in this, we use the `I` function in R to enforce the conditions on x. It is as below.
```{r 7 4, echo=TRUE}
x = seq(-2, 2, 0.05)
y = 1 + 1 * I(x <= 2 & x >= 0) - (x-1) * I(x <= 2 & x >= 1)  + 3 * (x-3) * I (x <= 4 & x >= 3) + I(x <= 5 & x>4)

plot(x, y)

# y-intercept
y[which(x==0)]
```

The y-intercept is 2 (`y[which(x==0)]`). Slope is -1 for $1 \le X \le 2$ and 0 for $-2 \le X < 0$ and $0 < X \le 1$. The function is discontinuos at `x=0`

### 5. (10 points) Section 7.9, Page 299, question 6
### 6. (20 points) Section 7.9, Page 299, question 7
### 7. (10 points) Section 8.4, Page 332, question 1
### 8. (10 points) Section 8.4, Page 333-334, question 8