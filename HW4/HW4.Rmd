---
title: "DS502- HW3"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1. (10 points) Section 6.8, page 259, question 2

(a) iii. holds true. Lasso puts a budget constraint on the parameters which decreases the model variance and it reduces overfitting. However, when we put a constraint, the model bias increases. From the bias-variance trade-off concept we can say that the lasso regression will give better prediction when its increase in bias is less that its decrease in variance.

(b) iii. holds true. Like Lasso above, Ridge also puts a budget constraint on the parameters which decreases the model variance and it reduces overfitting. However, when we put a constraint, the model bias increases. Similarly as above, from the bias-variance trade-off concept we can say that the Ridge regression will give better prediction when its increase in bias is less that its decrease in variance.

(c) ii. holds true. Since non-linear methods are more flexible, they have higher variance than least squares regression but lower bias. Again, from the bias variance trade-off, if the increase on model variance is less than the decrease in bias, then non-linear model will have better prediction accuracy.

### 2. (20 points) Section 6.8, page264, question 11

(a)
```{r 6 11 a, echo=TRUE}
library(MASS)
library(leaps)
library(boot)
library(glmnet)
attach(Boston)

colSums(sapply(Boston, is.na))
# No NAs in the dataset

m.matrix <- model.matrix(crim~., data = Boston)[, -1]
reg.fit <- regsubsets(m.matrix, Boston$crim, nvmax = dim(Boston)[2]-1)

```





### 3. (10 points) Section 7.9, Page 298, question 3

We have $b_{1}(X)=X , b_{2}(X)=(X-1)^{2} * I(X \ge 1)$

For $X \ge 1, I(X \ge 1) = 1$, and $X < 1, I(X \ge 1) = 0$

Substituting $\hat\beta_0 = 1, \hat\beta_1 = 1, \hat\beta_2 = âˆ’2$ in

$Y =\beta_0 +\beta_1 b_1(X)+ \beta_2b_2(X) + \epsilon$

We get,

$\hat Y =1 + b_1(X) -2 b_2(X)$

For $X \ge 1, \hat Y =1 + X - 2(X - 1)^{2} = -1 + 5X - 2X^2$, and for $X < 1, \hat Y = 1 + X$

```{r 6.8 2, echo=TRUE}
x_lower = seq(-2, 1, by = 0.05)
x_upper = seq(1, 2, by = 0.05)

y_lower = 1 + x_lower
y_upper = -1 + 5 * x_upper - 2 * (x_upper ^ 2)

x <- c(x_lower, x_upper)
y <- c(y_lower, y_upper)

plot(x,y)
```

$Y = 1 + X, for X<1$
$Now, for X=0, Y = 1$

Therefore y-intercept is 1. Slope is 1 when $X<1$ and by taking derivative for $Y$ where $X\ge1$, we get slope as $5 - 4 X$

### 4. (10 points) Section 7.9, Page 298, question 4

Similary from above,
we can split the function into multiple domains. Since there are a lot of cuts in this, we use the `I` function in R to enforce the conditions on x. It is as below.
```{r 7 4, echo=TRUE}
x = seq(-2, 2, 0.05)
y = 1 + 1 * I(x <= 2 & x >= 0) - (x-1) * I(x <= 2 & x >= 1)  + 3 * (x-3) * I (x <= 4 & x >= 3) + I(x <= 5 & x>4)

plot(x, y)

# y-intercept
y[which(x==0)]
```

The y-intercept is 2 (`y[which(x==0)]`). Slope is -1 for $1 \le X \le 2$ and 0 for $-2 \le X < 0$ and $0 < X \le 1$. The function is discontinuos at `x=0`

### 5. (10 points) Section 7.9, Page 299, question 6
### 6. (20 points) Section 7.9, Page 299, question 7
### 7. (10 points) Section 8.4, Page 332, question 1
### 8. (10 points) Section 8.4, Page 333-334, question 8