---
title: "DS502- HW1"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1. 2.4 (question 1)

* (a) The sample size n is extremely large, and the number of predictors
p is small.

As the sample size is extremely large the certainty of the veracity of the sample mean is high. Therefore the variance will be low, since an unknown sample will not deviate a lot from the sample mean of a large sample.
The low number of predictors will prevent overfitting and thus reduce variance. Since we have two factors that reduce variance, in this case, a flexible statistical learning method is expected to be better because it will reduce the bias. The higher variance of more flexible learning method will be countered by the above two factors.

* (b) The number of predictors p is extremely large, and the number
of observations n is small.

The large number of predictors will tend to overfit and perform poor on unseen data, thus increasing variance. Similarly with less number of samples there could be a lot of variablility in the least sqaures which would mean higher variance for unseen data. Therefore in this case an inflexible learning method is expected to perform better since it will counter the high variance.

* (c) The relationship between the predictors and response is highly
non-linear.

Where the relationship between predictors and response is highly non-linear, an inflexible model will have high bias given that it won't be able to capture the complex relationship between the predictors and response. Therefore a flexible method is expected to perform better in this case since it will reduce the bias.

* (d) The variance of the error terms, i.e. $\mu^2$, is extremely
high.

As the variance of error terms is extremely high, the certainty of the veracity of the sample mean is very low. Therefore the model variance will be high, since an unknown sample could deviate a lot from the sample mean. Hence, in this case, an inflexible statistical learning method is expected to be better because it will reduce the variance.


### 2. 2.4 (question 3)

```{r 2.4 a, echo=FALSE}
curve(150*x, from=1, to=9, xlab="flexibility", ylab="", col="white")

# (1) bias
curve(2000 * exp(-(x*0.8)) +100, add=TRUE, col="cyan2", lwd=2)
# (2) variance
curve(exp(x/1.5)+150, add=TRUE, col="orange", lwd=2)
# (3) irreducible error
curve(0*x+250, add=TRUE, col="gray", lwd=2, lty=2)                  

# training error (1+3)
curve(2000 * exp(-(x*0.8)) +100+0*x+250, add=TRUE, col="blue", lwd=2)
# MSE test error (1+2)
curve(2000 * exp(-(x*0.8)) +100+exp(x/1.5)+150, add=TRUE, col="red", lwd=2)

legend(10, legend=c("Bias", "Variance", "Train Error",
                    "Test Error", "Irreducible Error"),
       col=c("blue", "orange", "blue", "red", "gray"), lty=1:2, cex=0.3)
```

+ Bias is high for low flexibily as the model is not flexible enough to capture the complexities in the data. As the model becomes more flexible it tries to fit all the data and bias decreases until it reduces to irreducible error.
+ Variance is small when the model is simple, with small change in data there is small change in error. In the beginning, Variance does not increase significantly with increase in model complexity for when the model complexity <= true data complexity. When the model becomes too complex, the Variance increases exponentially with increase in flexibility.
+ Training error decreases with increase in model flexibily, similarly to the Bias. It is the sum of reducible and irreducible error.
+ Test error is lowest when the model complexity is closest to true data complexity. When the model is too simple, test error is high due to high bias, on the other hand when the model is too flexible, test error is high due to high variance.
+ Irreducible error is constant and independent of the model flexibily.


### 3. 2.4 (question 6)

A parametric statistical learning approach assumes a given form for $f$ and reduces the problem to estimate only the parameters of this form in order to predict $f$, however, a non-parametric approach does not assume any form for $f$ and hence requires a large set of data in order to estimate $f$.

The advantages of a parametric approach in regression or classification is the simplification of the model which could lead to a good estimation without requiring a large amount of data.

On the other side, this approach could lead to an erroneous model if the initially assumed form is wrong. Another problem could sometimes occur if we try to make the model more flexible until it overfits the training set.

### 4. 2.4 (question 8)

* (a) Reading `College.csv` into `college` variable
```{r 2.4 8a, echo=TRUE}
college <- read.csv("College.csv", stringsAsFactors = TRUE)
```

* (b) Set first column as row names and then remove that column from data
```{r 2.4 8b, echo=TRUE}
# fix(college)
rownames(college) = college[,1]
college = college[,-1]
# fix(college)
```

* (c) 
i. Summary of `college` variable
```{r 2.4 8c i, echo=TRUE}
summary(college)
```

ii. Scatterplot matrix of first 10 variables
```{r 2.4 8c ii, echo=TRUE}
pairs(college[,1:10])
```

iii. Boxplot of `Outstate` versus `Private`
```{r 2.4 8c iii, echo=TRUE, fig.align='center'}
plot(Outstate~Private, data=college)
```

iv. Elite Universities
```{r 2.4 8c iv, echo=TRUE, fig.align='center'}
Elite = rep("No",nrow(college))
Elite[college$Top10perc > 50] = " Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college)
plot(Outstate~Elite, data=college)
```

From the `summary`, we have **78 `Elite`** universities.

v. Histograms
```{r 2.4 8c v, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
par(mfrow=c(2,2))

hist_vars   = c("Apps", "S.F.Ratio", "Room.Board", "Grad.Rate")
hist_breaks = c(50, 10, 20, 20)
hist_data = subset(college, select = hist_vars)
make_hist <- function(list.elem, names, breaks) {
  hist(list.elem, main = names, xlab = names, breaks = breaks)
}
mapply(make_hist, 
       list.elem = hist_data, 
       names = names(hist_data),
       breaks = hist_breaks)
```

vi. Exploration
```{r 2.4 8c vi, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
college$Accept.Rate = college$Accept / college$Apps * 100
college$Enroll.Rate = college$Enroll / college$Accept * 100

par(mfrow=c(2,2))
plot(Grad.Rate ~ Outstate, data = college)
plot(Outstate ~ Private, data = college)
plot(Outstate ~ S.F.Ratio, data = college)
plot(S.F.Ratio ~ Private, data = college)

par(mfrow=c(2,2))
plot(Apps ~ Private, data = college)
plot(Enroll.Rate ~ Outstate, data = college)
```

Observations:

  + Strong positive correlation between `Grad.Rate` and `Outstate` fees
  + Significant difference in `Outstate` fees depending on if the university is `Private` and on the `S.F.Ratio`
  + From the second point and above plot, we see that `Private` university colleges have a smaller `S.F.Ratio`
  + `Private` university colleges tend to get lot more applications than public
  + `Enroll.Rate` is negatively correlated with the college `Outstate` fees

### 5. 2.4 (question 9)

```{r 2.4 9 setup, echo=TRUE}
library(ISLR)
data(Auto)
colSums(sapply(Auto, is.na))
```
No missing values

* (a) Qualitative and quantitative predictors
```{r 2.4 9a, echo=TRUE, fig.align='center'}
summary(Auto)
hist(Auto$origin, xlab = "origin", main = "Histogram of origin")
```
From the summary, we have
    
  + Qualitative predictors: name, origin
  + Quantitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year

`origin` is a qualitative predictor and no quantitative. Origin of car (1. American, 2. European, 3. Japanese)

* (b) Range of quantitative predictors
```{r 2.4 9b, echo=TRUE, fig.align='center', tidy=TRUE}
quantitative_preds = c("mpg", "cylinders", "displacement", "horsepower", "weight",
                       "acceleration", "year")
qualitative_preds = c("origin", "name")

ranges = data.frame(lapply(subset(Auto, select = quantitative_preds), range))
knitr::kable(ranges)
```

* (c) Mean and standard deviation of quantitative predictors
```{r 2.4 9c, echo=TRUE, fig.align='center'}
mean_sd = data.frame(lapply(subset(Auto, select = quantitative_preds), function(x) {
  c(mean = mean(x), std_dev = sd(x))
  }
))
knitr::kable(mean_sd)
```

* (d) Range, mean and standard deviation of quantitative predictors of **subset data**
```{r 2.4 9d, echo=TRUE, fig.align='center'}
subset_mean_sd = data.frame(lapply(subset(Auto[- c(10:85),], select = quantitative_preds),
                                   function(x) {
                                     c(range = range(x), mean = mean(x), std_dev = sd(x))
                                     }
                                   )
                            )
knitr::kable(subset_mean_sd)
```

* (e) Predictor Findings
```{r 2.4 9e, echo=TRUE, fig.height=10, fig.width=10, fig.align='center'}
factor_vars = c("origin")
newAuto = Auto
newAuto[factor_vars] = lapply(Auto[factor_vars], as.factor)
pairs(newAuto)
```

From the scatter plots, we observer that:

+ `mpg` decreases non-linearly with increase in displacement
+ Over time, `mpg` has shown increase
+ `displacement`, `horsepower`, `weight` are all highly correlated with each other
+ `origin:2` European and `origin:3` Japanese cars have smaller displacement compared to `origin:2` American cars

* (f) `mpg` prediction
```{r 2.4 9f, echo=TRUE, fig.height=3}
par(mfrow = c(1,3))
plot(mpg ~ cylinders + displacement + horsepower + weight + acceleration +
       year + origin, data = newAuto)

fit = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration +
           year + origin, data = newAuto)
summary(fit)
```


From the plots and the fit summary:

  + `origin` is a very good predictor of `mpg` since the coefficients are large with small Std. Error (p-stat less than 0.05)
  + The next significant predictor is `year` with low p-stat value. We also see in the plot that `mpg` tends to increase year on year
  + `weight` is significant predictor but has a small coefficient, i.e. very small effect on the predictions of `mpg`
  + `horsepower`, `displacement` and `cylinders` cannot be used as predictor for `mpg` since the their estimated coefficients and Std. Error are close, consequently the coefficients might very well be zero, which is suggested by (p-stat >> 0.05)



### 6. 3.7 (question 1)

The small p-values in Table 3.4 for TV and radio indicate that, holding the other predictors constant, each of these predictors will, with high likelihood, change the amount of sales. For example, fixing the amount of TV and newspaper advertising, while increasing the amount of radio advertising, will very likely lead to an increase in sales because the p-value in Table 3.4 is very small for radio and radio's coefficient is large and positive.

Similarly, fixing the amount of radio and newspaper advertising, while increasing the amount of TV advertising, will very likely lead to an increase in sales because the p-value in Table 3.4 is very small for TV and radio's coefficient is positive.

One unit increase in TV (while keeping other variables constant) will likely lead to smaller increase in sales than one unit increase in radio (while keeping other variables constant) as the coeff(radio) > coeff(TV).

On the other hand, since the p-value for newspaper is quite large, the data indicates that newspaper advertising in unlikely to have any effect on sales when TV and radio are held fixed. Note, Table 3.3 does show a small p-value for a single linear regression across
newspaper and sales, but this is likely because newspaper advertising is predictive of TV and radio advertising, not necessarily because newspaper advertising directly influences sales.

### 7. 3.7 (question 5)

Given that, 

$\hat{y_{i}} = x_{i} \hat{\beta}$ and $\hat{\beta} = \frac{\sum_{i'=1}^n x_{i'} y_{i'}}{\sum_{j=1}^n x_{j}^2}$

We can write,

$\hat{y_{i}} = x_i \frac{\sum_{i'=1}^n x_{i'} y_{i'}}{\sum_{j=1}^n x_{j}^2} = \sum_{i'=1}^n\left( \frac{x_{i'} x_i}{\sum_{j=1}^n x_{j}^2}\right) y_{i'}$

Hence,

$\hat{y_{i}} = \sum_{i'=1}^n{a_{i'} y_{i'}}$ where $a_{i'} = \frac{x_{i'} x_i}{\sum_{j=1}^n x_{j}^2}$

### 8. 3.7 (question 6)

On one hand, we know from (3.2) that the least squares line respects this equation

$\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$

On the other hand, we know from (3.4) that

$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$

Hence,

$\hat{y} = (\bar{y} - \hat{\beta_1} \bar{x}) + \hat{\beta_1} x$

For $x = \bar{x}$,

$\hat{y} = \bar{y} - \hat{\beta_1} \bar{x} + \hat{\beta_1} \bar{x} = \bar{y}$

Consquently, the point $(\bar{x} , \bar{y})$ belongs to the least squares line.

### 9. 3.7 (question 8)

* (a) The following code performs a simple linear regression on the Auto dataset

```{r 3.7 8a, echo=TRUE}

# Import the ISLR package
library(ISLR)

# Attach the Auto as the current database
attach(Auto)

# Perform a simple linear regression
lmFit=lm(mpg~horsepower)

#Print the results
summary(lmFit)

# Calculate the mean of the response (mpg) (question ii.)
mean(mpg)
```

i. Given that the p-value of the F-statistic is close to zero and the F-statistic is larger than 1, we can reject the null-hypothesis stating that all regression coefficients are equal to zero, therefore, we can conclude that there is a significant relationship between horsepower and mpg.

ii. First, the mean value for the response, which is mpg, is 23.446 and since the RSE of lmFit was 4.906, we can observe that the percentage error is about `r round(4.906*100/mean(mpg), digits=3)`%.
Second, the $R^2$ of lmFit was about 0.6059 which reflects that 60.59% of the variance in mpg is explained by the predictor horsepower.

iii. The relationship between the predictor and the response is negative meaning that the more horsepower a given car would have the less mpg efficiency it would have.

iv. 
``` {r 3.7 8a iv}
predict(lmFit, data.frame(horsepower=98), interval="confidence")
predict(lmFit, data.frame(horsepower=98), interval="prediction")
```

The predicted mpg associated with a horsepower of 98 is 24.46708 with a confidence interval of [23.97308, 24.96108] and a prediction interval of [14.8094, 34.12476]

* (b) 
``` {r 3.7 8b}
# Plot the response and the predictor
plot(horsepower, mpg)

# Add the least squares regression line
abline(lmFit)
```

* (c) 
``` {r 3.7 8c}
plot(lmFit)
```

According to the residuals plots, we can observe that they are highly biased at lower and higher values and respectively less biased in the middle. We can note then some signs of non-linearity.
In addition, the plot of residuals vs leverage reveals some high leverage points and some outliers outside the range [-2,2]

### 10. 3.7 (question 9)

* (a)

``` {r 3.7 9a}
# Produce a scatterplot
pairs(Auto)
```

* (b)

``` {r 3.7 9b}
# Remove the names variable
names(Auto)

# Compute the correlation matrix
cor(Auto[1:8])
```

* (c)
``` {r 3.7 9c}
lmFit2 = lm(mpg ~ . - name, data = Auto)
summary(lmFit2)
```

i. We can observe that the p-value of the F-statistic is close to zero and the F-statistic is greater than 1, hence, we can reject the null-hypothesis and confirm the relationship between `mpg` and the other predictors.

ii. By referring to the p-values associated with each predictor's t-statistic, we can observe that the variables `year`, `weight`, `origin`, `displacement` are statistically significant to the response.

iii. The coefficient of the `year` variable suggests that an increase of 1 year (the car is a year newer) involves a 0.75 mile per gallon increase (car is more fuel efficient) while the other predictors remain constant.

* (d)
``` {r 3.7 9d}
# 2 x 2 pictures on one plot
par(mfrow = c(2, 2))
# 
plot(lmFit2)
```

The residuals vs fitted graph shows a slight non-linearity in the relationship between the response and the predictors. We can also observe that the residuals seem to be heteroscedastic (as the fitted values increase, the range of the residuals increases).
We notice that the residuals vs leverage plot suggests the presence of some outliers (outside the interval [-2,2]) and the presence of some observations with unusually high leverage (such as point 14)

* (e)

``` {r 3.7 9e i}
lmFit2 = lm(mpg ~ year + weight + origin + year:origin, data = Auto)
summary(lmFit2)
```

In this model, we tried to combine the most significant predictors and add an interaction between `year` and `origin`.
The results show an improvement in the adjusted R-squared (we use the adjusted R-squared since the two models don't have the same number of predictors) which reflects that the new model explains better the variablity in `mpg`.
We notice also that the value of the p-value for the interaction term is close to zero which explains that the term is significant in the model.


``` {r 3.7 9e ii}
lmFit2 = lm(mpg ~.-name+horsepower:weight, data = Auto)
summary(lmFit2)
```

In this model, we take the initial model and add an interaction between `horsepower` and `weight`.
The results show that the variability in `mpg` is even better explained than the previous model (85.9% for adjusted R-squared compared to 82.13% in the previous model)..
The interaction term `horsepower:weight` has a low p-value which means that it is statistically significant to the response.

* (f)

``` {r 3.7 9f i}
lmFit2 = lm(mpg ~.-name+ log(horsepower), data = Auto)
summary(lmFit2)
```

``` {r 3.7 9f ii}
lmFit2 = lm(mpg ~.-name+ I((weight)^2), data = Auto)
summary(lmFit2)
```

``` {r 3.7 9f iii}
lmFit2 = lm(mpg ~.-name+ log(horsepower) + I((horsepower)^2), data = Auto)
summary(lmFit2)
```

``` {r 3.7 9f iv}
lmFit2 = lm(mpg ~.-name+ log(displacement) + I((cylinders)^2), data = Auto)
summary(lmFit2)
```

We notice in the previous models, that the value of adjusted R-squared has slightly increased and that all the new added terms are statistically significant. The RSS value is almost stable.

However, this absence of big improvement could be explained by the fact that the relationship between `mpg` and the predictors could be closer to a linear model, which explains the ineffectiveness of the application of non-linear models.
