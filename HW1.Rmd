---
title: "DS502- HW1"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1. 2.4 (question 1)

* (a) The sample size n is extremely large, and the number of predictors
p is small.

As the sample size is extremely large the certainty of the veracity of the sample mean is high. Therefore the variance will be low, since an unknown sample will not deviate a lot from the sample mean of a large sample.
The low number of predictors will prevent overfitting and thus reduce variance. Since we have two factors that reduce variance, in this case, a flexible statistical learning method is expected to be better because it will reduce the bias. The higher variance of more flexible learning method will be countered by the above two factors.

* (b) The number of predictors p is extremely large, and the number
of observations n is small.

The large number of predictors will tend to overfit and perform poor on unseen data, thus increasing variance. Similarly with less number of samples there could be a lot of variablility in the least sqaures which would mean higher variance for unseen data. Therefore in this case an inflexible learning method is expected to perform better since it will counter the high variance.

* (c) The relationship between the predictors and response is highly
non-linear.

Where the relationship between predictors and response is highly non-linear, an inflexible model will have high bias given that it won't be able to capture the complex relationship between the predictors and response. Therefore a flexible method is expected to perform better in this case since it will reduce the bias.

* (d) The variance of the error terms, i.e. $\mu^2$ = Var(), is extremely
high.

As the variance of error terms is extremely high, the certainty of the veracity of the sample mean is very low. Therefore the model variance will be high, since an unknown sample could deviate a lot from the sample mean. Hence, in this case, an inflexible statistical learning method is expected to be better because it will reduce the variance.


### 2. 2.4 (question 3)

### 3. 2.4 (question 6)

### 4. 2.4 (question 8)

* (a) Reading `College.csv` into `college` variable
```{r 2.4 8a, echo=TRUE}
college <- read.csv("College.csv", stringsAsFactors = TRUE)
```

* (b) Set first column as row names and then remove that column from data
```{r 2.4 8b, echo=TRUE}
# fix(college)
rownames(college) = college[,1]
college = college[,-1]
# fix(college)
```

* (c) 
i. Summary of `college` variable
```{r 2.4 8c i, echo=TRUE}
summary(college)
```

ii. Scatterplot matrix of first 10 variables
```{r 2.4 8c ii, echo=TRUE}
pairs(college[,1:10])
```

iii. Boxplot of `Outstate` versus `Private`
```{r 2.4 8c iii, echo=TRUE, fig.align='center'}
plot(Outstate~Private, data=college)
```

iv. Elite Universities
```{r 2.4 8c iv, echo=TRUE, fig.align='center'}
Elite = rep("No",nrow(college))
Elite[college$Top10perc > 50] = " Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college)
plot(Outstate~Elite, data=college)
```

From the `summary`, we have **78 `Elite`** universities.

v. Histograms
```{r 2.4 8c v, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
par(mfrow=c(2,2))

hist_vars   = c("Apps", "S.F.Ratio", "Room.Board", "Grad.Rate")
hist_breaks = c(50, 10, 20, 20)
hist_data = subset(college, select = hist_vars)
make_hist <- function(list.elem, names, breaks) {
  hist(list.elem, main = names, xlab = names, breaks = breaks)
}
mapply(make_hist, list.elem = hist_data, names = names(hist_data), breaks = hist_breaks)
```

vi. Exploration
```{r 2.4 8c vi, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
college$Accept.Rate = college$Accept / college$Apps * 100
college$Enroll.Rate = college$Enroll / college$Accept * 100

par(mfrow=c(2,2))
plot(Grad.Rate ~ Outstate, data = college)
plot(Outstate ~ Private, data = college)
plot(Outstate ~ S.F.Ratio, data = college)
plot(S.F.Ratio ~ Private, data = college)

par(mfrow=c(2,2))
plot(Apps ~ Private, data = college)
plot(Enroll.Rate ~ Outstate, data = college)
```

Observations:

  + Strong positive correlation between `Grad.Rate` and `Outstate` fees
  + Significant difference in `Outstate` fees depending on if the university is `Private` and on the `S.F.Ratio`
  + From the second point and above plot, we see that `Private` university colleges have a smaller `S.F.Ratio`
  + `Private` university colleges tend to get lot more applications than public
  + `Enroll.Rate` is negatively correlated with the college `Outstate` fees

### 5. 2.4 (question 9)

```{r 2.4 9 setup, echo=TRUE}
library(ISLR)
data(Auto)
colSums(sapply(Auto, is.na))
```
No missing values

* (a) Qualitative and quantitative predictors
```{r 2.4 9a, echo=TRUE, fig.align='center'}
summary(Auto)
hist(Auto$origin, xlab = "origin", main = "Histogram of origin")
```
From the summary, we have
    
  + Qualitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year
  + Quantitative predictors: name, origin

`origin` is a qualitative predictor and no quantitative. Origin of car (1. American, 2. European, 3. Japanese)

* (b) Range of quantitative predictors
```{r 2.4 9b, echo=TRUE, results="asis", fig.align='center'}
quantitative_preds = c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
qualitative_preds = c("origin", "name")

ranges = data.frame(lapply(subset(Auto, select = quantitative_preds), range))
knitr::kable(ranges)
```

* (c) Mean and standard deviation of quantitative predictors
```{r 2.4 9c, echo=TRUE, fig.align='center'}
mean_sd = data.frame(lapply(subset(Auto, select = quantitative_preds), function(x) {
  c(mean = mean(x), std_dev = sd(x))
  }
))
knitr::kable(mean_sd)
```

* (d) Range, mean and standard deviation of quantitative predictors of **subset data**
```{r 2.4 9d, echo=TRUE, fig.align='center'}
subset_mean_sd = data.frame(lapply(subset(Auto[- c(10:85),], select = quantitative_preds), function(x) {
  c(range = range(x), mean = mean(x), std_dev = sd(x))
  }
))
knitr::kable(subset_mean_sd)
```

* (e) Predictor Findings
```{r 2.4 9e, echo=TRUE, fig.height=10, fig.width=10, fig.align='center'}
factor_vars = c("origin", "cylinders", "name", "year")
Auto[factor_vars] = lapply(Auto[factor_vars], as.factor)
pairs(Auto)
```
From the scatter plots, we observer that:

+ `mpg` decreases non-linearly with increase in displacement
+ Over time, `mpg` has shown increase
+ `displacement`, `horsepower`, `weight` are all highly correlated with each other
+ `origin:2` European and `origin:3` Japanese cars have smaller displacement compared to `origin:2` American cars

* (f) `mpg` prediction
```{r 2.4 9f, echo=TRUE, fig.height=3}
par(mfrow = c(1,3))
plot(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = Auto)

fit = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration + year + origin, data = Auto)
summary(fit)
```
