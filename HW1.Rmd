---
title: "DS502- HW1"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60), tidy=TRUE)
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1. 2.4 (question 1)

* (a) The sample size n is extremely large, and the number of predictors
p is small.

As the sample size is extremely large the certainty of the veracity of the sample mean is high. Therefore the variance will be low, since an unknown sample will not deviate a lot from the sample mean of a large sample.
The low number of predictors will prevent overfitting and thus reduce variance. Since we have two factors that reduce variance, in this case, a flexible statistical learning method is expected to be better because it will reduce the bias. The higher variance of more flexible learning method will be countered by the above two factors.

* (b) The number of predictors p is extremely large, and the number
of observations n is small.

The large number of predictors will tend to overfit and perform poor on unseen data, thus increasing variance. Similarly with less number of samples there could be a lot of variablility in the least sqaures which would mean higher variance for unseen data. Therefore in this case an inflexible learning method is expected to perform better since it will counter the high variance.

* (c) The relationship between the predictors and response is highly
non-linear.

Where the relationship between predictors and response is highly non-linear, an inflexible model will have high bias given that it won't be able to capture the complex relationship between the predictors and response. Therefore a flexible method is expected to perform better in this case since it will reduce the bias.

* (d) The variance of the error terms, i.e. $\mu^2$ = Var(), is extremely
high.

As the variance of error terms is extremely high, the certainty of the veracity of the sample mean is very low. Therefore the model variance will be high, since an unknown sample could deviate a lot from the sample mean. Hence, in this case, an inflexible statistical learning method is expected to be better because it will reduce the variance.


### 2. 2.4 (question 3)

### 3. 2.4 (question 6)

### 4. 2.4 (question 8)

* (a) Reading `College.csv` into `college` variable
```{r 2.4 8a, echo=TRUE}
college <- read.csv("College.csv", stringsAsFactors = TRUE)
```

* (b) Set first column as row names and then remove that column from data
```{r 2.4 8b, echo=TRUE}
# fix(college)
rownames(college) = college[,1]
college = college[,-1]
# fix(college)
```

* (c) 
i. Summary of `college` variable
```{r 2.4 8c i, echo=TRUE}
summary(college)
```

ii. Scatterplot matrix of first 10 variables
```{r 2.4 8c ii, echo=TRUE}
pairs(college[,1:10])
```

iii. Boxplot of `Outstate` versus `Private`
```{r 2.4 8c iii, echo=TRUE, fig.align='center'}
plot(Outstate~Private, data=college)
```

iv. Elite Universities
```{r 2.4 8c iv, echo=TRUE, fig.align='center'}
Elite = rep("No",nrow(college))
Elite[college$Top10perc > 50] = " Yes"
Elite = as.factor(Elite)
college = data.frame(college, Elite)
summary(college)
plot(Outstate~Elite, data=college)
```

From the `summary`, we have **78 `Elite`** universities.

v. Histograms
```{r 2.4 8c v, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
par(mfrow=c(2,2))

hist_vars   = c("Apps", "S.F.Ratio", "Room.Board", "Grad.Rate")
hist_breaks = c(50, 10, 20, 20)
hist_data = subset(college, select = hist_vars)
make_hist <- function(list.elem, names, breaks) {
  hist(list.elem, main = names, xlab = names, breaks = breaks)
}
mapply(make_hist, 
       list.elem = hist_data, 
       names = names(hist_data),
       breaks = hist_breaks)
```

vi. Exploration
```{r 2.4 8c vi, echo=TRUE, results='hide', fig.height= 8, fig.align='center'}
college$Accept.Rate = college$Accept / college$Apps * 100
college$Enroll.Rate = college$Enroll / college$Accept * 100

par(mfrow=c(2,2))
plot(Grad.Rate ~ Outstate, data = college)
plot(Outstate ~ Private, data = college)
plot(Outstate ~ S.F.Ratio, data = college)
plot(S.F.Ratio ~ Private, data = college)

par(mfrow=c(2,2))
plot(Apps ~ Private, data = college)
plot(Enroll.Rate ~ Outstate, data = college)
```

Observations:

  + Strong positive correlation between `Grad.Rate` and `Outstate` fees
  + Significant difference in `Outstate` fees depending on if the university is `Private` and on the `S.F.Ratio`
  + From the second point and above plot, we see that `Private` university colleges have a smaller `S.F.Ratio`
  + `Private` university colleges tend to get lot more applications than public
  + `Enroll.Rate` is negatively correlated with the college `Outstate` fees

### 5. 2.4 (question 9)

```{r 2.4 9 setup, echo=TRUE}
library(ISLR)
data(Auto)
colSums(sapply(Auto, is.na))
```
No missing values

* (a) Qualitative and quantitative predictors
```{r 2.4 9a, echo=TRUE, fig.align='center'}
summary(Auto)
hist(Auto$origin, xlab = "origin", main = "Histogram of origin")
```
From the summary, we have
    
  + Qualitative predictors: name, origin
  + Quantitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year

`origin` is a qualitative predictor and no quantitative. Origin of car (1. American, 2. European, 3. Japanese)

* (b) Range of quantitative predictors
```{r 2.4 9b, echo=TRUE, fig.align='center', tidy=TRUE}
quantitative_preds = c("mpg", "cylinders", "displacement", "horsepower", "weight",
                       "acceleration", "year")
qualitative_preds = c("origin", "name")

ranges = data.frame(lapply(subset(Auto, select = quantitative_preds), range))
knitr::kable(ranges)
```

* (c) Mean and standard deviation of quantitative predictors
```{r 2.4 9c, echo=TRUE, fig.align='center'}
mean_sd = data.frame(lapply(subset(Auto, select = quantitative_preds), function(x) {
  c(mean = mean(x), std_dev = sd(x))
  }
))
knitr::kable(mean_sd)
```

* (d) Range, mean and standard deviation of quantitative predictors of **subset data**
```{r 2.4 9d, echo=TRUE, fig.align='center'}
subset_mean_sd = data.frame(lapply(subset(Auto[- c(10:85),], select = quantitative_preds),
                                   function(x) {
                                     c(range = range(x), mean = mean(x), std_dev = sd(x))
                                     }
                                   )
                            )
knitr::kable(subset_mean_sd)
```

* (e) Predictor Findings
```{r 2.4 9e, echo=TRUE, fig.height=10, fig.width=10, fig.align='center'}
factor_vars = c("origin")
Auto[factor_vars] = lapply(Auto[factor_vars], as.factor)
pairs(Auto)
```

From the scatter plots, we observer that:

+ `mpg` decreases non-linearly with increase in displacement
+ Over time, `mpg` has shown increase
+ `displacement`, `horsepower`, `weight` are all highly correlated with each other
+ `origin:2` European and `origin:3` Japanese cars have smaller displacement compared to `origin:2` American cars

* (f) `mpg` prediction
```{r 2.4 9f, echo=TRUE, fig.height=3}
par(mfrow = c(1,3))
plot(mpg ~ cylinders + displacement + horsepower + weight + acceleration +
       year + origin, data = Auto)

fit = lm(mpg ~ cylinders + displacement + horsepower + weight + acceleration +
           year + origin, data = Auto)
summary(fit)
```


From the plots and the fit summary:

  + `origin` is a very good predictor of `mpg` since the coefficients are large with small Std. Error (p-stat less than 0.05)
  + The next significant predictor is `year` with low p-stat value. We also see in the plot that `mpg` tends to increase year on year
  + `weight` is significant predictor but has a small coefficient, i.e. very small effect on the predictions of `mpg`
  + `horsepower`, `displacement` and `cylinders` cannot be used as predictor for `mpg` since the their estimated coefficients and Std. Error are close, consequently the coefficients might very well be zero, which is suggested by (p-stat >> 0.05)



### 6. 3.7 (question 1)



### 7. 3.7 (question 5)

Given that, 

$\hat{y_{i}} = x_{i} \hat{\beta}$ and $\hat{\beta} = \frac{\sum_{i'=1}^n x_{i'} y_{i'}}{\sum_{j=1}^n x_{j}^2}$

We can write,

$\hat{y_{i}} = x_i \frac{\sum_{i'=1}^n x_{i'} y_{i'}}{\sum_{j=1}^n x_{j}^2} = \sum_{i'=1}^n\left( \frac{x_{i'} x_i}{\sum_{j=1}^n x_{j}^2}\right) y_{i'}$

Hence,

$\hat{y_{i}} = \sum_{i'=1}^n{a_{i'} y_{i'}}$ where $a_{i'} = \frac{x_{i'} x_i}{\sum_{j=1}^n x_{j}^2}$

### 8. 3.7 (question 6)

On one hand, we know from (3.2) that the least squares line respects this equation

$\hat{y} = \hat{\beta_0} + \hat{\beta_1} x$

On the other hand, we know from (3.4) that

$\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}$

Hence,

$\hat{y} = (\bar{y} - \hat{\beta_1} \bar{x}) + \hat{\beta_1} x$

For $x = \bar{x}$,

$\hat{y} = \bar{y} - \hat{\beta_1} \bar{x} + \hat{\beta_1} \bar{x} = \bar{y}$

Consquently, the point $(\bar{x} , \bar{y})$ belongs to the least squares line.

### 9. 3.7 (question 8)

* (a) The following code performs a simple linear regression on the Auto dataset

```{r 8 a, echo=TRUE}

# Import the ISLR package
library(ISLR)

# Attach the Auto as the current database
attach(Auto)

# Perform a simple linear regression
lmFit=lm(mpg~horsepower)

#Print the results
summary(lmFit)

# Calculate the mean of the response (mpg) (question ii.)
mean(mpg)
```

i. Given that the p-value of the F-statistic is close to zero and the F-statistic is large than 1, we can reject the null-hypothesis stating that all regression coefficients are equal to zero, therefore, we can conclude that there is a significant relationship between horsepower and mpg.

ii. First, the mean value for the response, which is mpg, is 23.446 and since the RSE of lmFit was 4.906, we can observe that the percentage error is about `r round(4.906*100/mean(mpg), digits=3)`%.
Second, the $R^2$ of lmFit was about 0.6059 which reflects that 60.59% of the variance in mpg is explained by the predictor horsepower.

iii. The relationship between the predictor and the response is negative meaning that the more horsepower a given car would have the less mpg efficiency it would have.

iv. 
``` {r 8 a iv}
predict(lmFit, data.frame(horsepower=98), interval="confidence")
predict(lmFit, data.frame(horsepower=98), interval="prediction")
```

The predicted mpg associated with a horsepower of 98 is 24.46708 with a confidence interval of [23.97308, 24.96108] and a prediction interval of [14.8094, 34.12476]

* (b) 
``` {r 8 b}
# Plot the response and the predictor
plot(horsepower, mpg)

# Add the least squares regression line
abline(lmFit)
```

* (c) 
``` {r 8 c}
plot(lmFit)
```

According to the residuals plots, we can observe that they are highly biased at lower and higher values and respectively less biased in the middle. We can note then some signs of non-linearity.
