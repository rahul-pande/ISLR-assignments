---
title: "DS502- HW2"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1.(5 points) Section 4.7, page 168, question 1
### 2.(10 points) Section 4.7, page 169-170, question 5

a. If the Bayes' decision boundary is linear, QDA is expected to perform better since it is more flexible and will tend to overfit data. However, on the test data, LDA is expected to perform better since is expected to have lower variance than QDA. Owing to overfitting on the training data, QDA might perform worse on the test data than LDA

b. If the Bayes' decision boundary is non-linear, QDA is expected to perform better on both train and test data since LDA will underfit and wil have high bias.

c. QDA is more flexible than LDA. Therefore it suffers from higher variance. However, variance decreases with more data and the fit will be closer to the actual fit. Therefore, QDA is expected to have better test prediction accuracy with more number of samples.

d. False. Following the previous pint, when there are few sample points, QDA will suffer from high variance since it will overfit train data. Therefore, it will have higher test error compared to LDA.

### 3.(5 points) Section 4.7, page 170, question 8

In the case of **logistic regression**, error rate shot up on test data which is an indicator that the model has **high variance**. Therefore the model will tend to perform poor on unseen data. There may be many reasons for this. Perhaps the the decision boundary is not linear.

In the case of **1NN**, since all testing samples would be present in the training data and since the point closest to any point is the point itself, the training error is zero. This makes the testing error as 36% (with avg error rate as 18%).

Although the information provided is not quite sufficient, we should prefer to use logistic model with lower test error rate and less difference between training and test error rate (more generalized)

### 4.(15 points) Section 4.7, page171, question 10

a. 
```{r 4.7 10 a, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
require(ISLR)
data("Weekly")
data.weekly = Weekly
summary(data.weekly)

# standardize data
data.weekly[ , colnames(data.weekly) != "Direction"] = scale(data.weekly[ , colnames(data.weekly) != "Direction"])

# pair plot
pairs(data.weekly)

# box plots w.r.t. response variables
par(mfrow=c(3,3))

vars = setdiff(names(Weekly), c("Today", "Direction"))

plot_against_direction <- function(y){
  x = "Direction"
  f <- as.formula(paste(c(y, x), collapse = " ~ "))
  boxplot(f, data= Weekly, ylab = y, xlab = x)
}

sapply(vars, plot_against_direction)
```

+ `Year` is non-linearly related to `Volume`. At first, `Volume` increases rapidly with `Year` but then becomes constant.
+ From the shape of graphs of `Year` against `Lag` variables, the variability in `Lag` vars is higher in the middle and in the end `Year`
+ Similarly, the variablility in `Lag` variables is higher towards either end (high or low) of `Volume` than the middle region.

b. 

```{r 4.7 10 b, echo=TRUE}
logistic.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = data.weekly, family = "binomial")
summary(logistic.fit)
```
From the `summary` of the fit, we see that `Lag2` is a significant factor in predicting `Direction`

c. 

```{r 4.7 10 c, echo=TRUE}
pred <- round(predict(logistic.fit, data.weekly, type = "response"))
pred <- factor(x = pred, labels = c("Down", "Up"))
actual <- data.weekly$Direction
conf_matrix <- table(actual, pred)
message("Confusion matrix")
conf_matrix
message("Fraction of correct predictions:")
(conf_matrix[1] + conf_matrix[4])/sum(conf_matrix)
```

Consider that the logistic model is predicting "is the return positive (`Up`)?"
Now from the confusion matrix the model is making `430` **type 1** errors and `48` **type 2** errors

d. 

```{r 4.7 10 d, echo=TRUE}
train.data <- data.weekly[Weekly$Year <= 2008, ]
heldout.data <- data.weekly[Weekly$Year > 2008, ]

lag2.fit <- glm(Direction ~ Lag2, data = train.data, family = "binomial")

lag2_pred <- round(predict(lag2.fit, heldout.data, type = "response"))
lag2_pred <- factor(x = lag2_pred, labels = c("Down", "Up"))
lag2_actual <- heldout.data$Direction
lag2_conf_matrix <- table(lag2_actual, lag2_pred)
message("Confusion matrix for Lag2 model")
lag2_conf_matrix
message("Fraction of correct predictions:")
(lag2_conf_matrix[1] + lag2_conf_matrix[4])/sum(lag2_conf_matrix)

```

e.

```{r 4.7 10 e, echo=TRUE}
require(MASS)
lda.fit <- lda(Direction ~ Lag2, data = train.data)

lda_pred <- predict(lda.fit, heldout.data)$class
lda_actual <- heldout.data$Direction
lda_conf_matrix <- table(lda_actual, lda_pred)
message("Confusion matrix for lda model")
lda_conf_matrix
message("Fraction of correct predictions:")
(lda_conf_matrix[1] + lda_conf_matrix[4])/sum(lda_conf_matrix)
```

f.

```{r 4.7 10 f, echo=TRUE}

qda.fit <- qda(Direction ~ Lag2, data = train.data)

qda_pred <- predict(qda.fit, heldout.data)$class
qda_actual <- heldout.data$Direction
qda_conf_matrix <- table(qda_actual, qda_pred)
message("Confusion matrix for lda model")
qda_conf_matrix
message("Fraction of correct predictions:")
(qda_conf_matrix[1] + qda_conf_matrix[4])/sum(qda_conf_matrix)

```
g.

```{r 4.7 10 g, echo=TRUE}
require(class)

set.seed(123)

trainX <- as.matrix(train.data$Lag2)
testX <- as.matrix(heldout.data$Lag2)
knn_pred <- knn(trainX, testX, train.data$Direction, k=1)
knn_actual <- heldout.data$Direction
knn_conf_matrix <- table(knn_actual, knn_pred)
message("Confusion matrix for knn=1 model")
knn_conf_matrix
message("Fraction of correct predictions:")
(knn_conf_matrix[1] + knn_conf_matrix[4])/sum(knn_conf_matrix)
```

h. (i)
After experimenting with interaction variables, k and variable transformation, we conclude that Logistic Regression and LDA work best for the `Weekly` dataset.

### 5.(15 points) Section 4.7, page 171-172, question 11

a.
```{r 4.7 11 a, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
data("Auto")
Auto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto = Auto[, names(Auto) != "mpg"]
Auto$mpg01 <- as.factor(Auto$mpg01)

```

b.
```{r 4.7 11 b, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
pairs(Auto)

par(mfrow=c(3,3))

vars = setdiff(names(Auto), c("mpg01", "name"))

plot_against_mpg01 <- function(y){
  x = "mpg01"
  f <- as.formula(paste(c(y, x), collapse = " ~ "))
  boxplot(f, data = Auto, ylab = y, xlab = x)
}

sapply(vars, plot_against_mpg01)

hist(Auto$cylinders[Auto$mpg01 == 0 ], xlab = "cylinders (mpg == 0)", main = "")
hist(Auto$cylinders[Auto$mpg01 == 1 ], xlab = "cylinders (mpg == 1)", main = "")
```

From the plots:

  + From the boxplot of `cylinders` vs `mpg01`, `cylinders` is probably the most important predictor of `mpg`. Most of the `1` class are at 4 cylinders.
  + `weight` could be a good predictor of `mpg01` since the boxplot shows that the medians are quite apart and overlap is small. Similarly, `displacement` and `horsepower` could also be good predictors.
  + There is strong correlation between variables like `weight`, `horsepower`, `displacement`. So there's probably redundant information in these variables.
  
c.
```{r 4.7 11 c, echo=TRUE}
perc_train <- 0.8
n_rows <- nrow(Auto)
n_train <- round(perc_train * n_rows)

samples = sample(1:n_rows, n_train, replace=FALSE)
train = logical(n_rows)
train[samples] = TRUE
test = !train

train.data <- Auto[train, ]
test.data <- Auto[test, ]

dim(train.data)
dim(test.data)
```

d.
```{r 4.7 11 d, echo=TRUE}
lda.fit <- lda(mpg01 ~ . -name -year -acceleration, data = train.data)

lda_pred <- predict(lda.fit, test.data)$class
lda_actual <- test.data$mpg01
lda_conf_matrix <- table(lda_actual, lda_pred)
message("Confusion matrix for lda model")
lda_conf_matrix
message("Error rate of LDA:")
(lda_conf_matrix[2] + lda_conf_matrix[3])/sum(lda_conf_matrix)
```

e.
```{r 4.7 11 e, echo=TRUE}
qda.fit <- qda(mpg01 ~ . -name -year -acceleration, data = train.data)

qda_pred <- predict(qda.fit, test.data)$class
qda_actual <- test.data$mpg01
qda_conf_matrix <- table(qda_actual, qda_pred)
message("Confusion matrix for lda model")
qda_conf_matrix
message("Error rate of QDA:")
(qda_conf_matrix[2] + qda_conf_matrix[3])/sum(qda_conf_matrix)
```

f. 
```{r 4.7 11 f, echo=TRUE}
consider.columns = !(names(train.data) %in% c("name", "year", "acceleration"))
logistic.fit <- glm(mpg01 ~ ., data = train.data[, consider.columns], family = "binomial")
summary(logistic.fit)

pred <- round(predict(logistic.fit, test.data[, consider.columns], type = "response"))
pred <- factor(x = pred, labels = c("0", "1"))
actual <- test.data$mpg01
conf_matrix <- table(actual, pred)
message("Confusion matrix")
conf_matrix
message("Error rate for logistic regression:")
(conf_matrix[2] + conf_matrix[3])/sum(conf_matrix)
```

g.
```{r 4.7 11 g, echo=TRUE}
knn_features <- !(names(train.data) %in% c("name", "year", "acceleration", "mpg01"))
trainX <- as.matrix(train.data[, knn_features])
testX <- as.matrix(test.data[, knn_features])

calculate_knn<- function(i) {
  set.seed(123)
  knn_pred <- knn(trainX, testX, train.data$mpg01, k=i)
  knn_actual <- test.data$mpg01
  knn_conf_matrix <- table(knn_actual, knn_pred)
  # sprintf("Confusion matrix for knn=1 model")
  # print(knn_conf_matrix)
  error_rate <- (knn_conf_matrix[2] + knn_conf_matrix[3])/sum(knn_conf_matrix)
  return(c(i, error_rate))
}

k_data <- data.frame(t(sapply(seq(from=1, to=51, by=2), calculate_knn)))
colnames(k_data) <- c("k", "error_rate")
plot(error_rate ~ k, data=k_data)

best_k <- k_data$k[k_data$error_rate == min(k_data$error_rate)]
best_k <- paste(as.vector(best_k), collapse = ", ")

sprintf("From the plot, k=%s seems to perform better on this dataset", best_k)
```


### 6.(10 points) Section 5.4, page 197, question 1
### 7.(10 points) Section 5.4, page 197, question 2
### 8.(15 points) Section 5.4, page 198, question 5
### 9.(15 points) Section 5.4, page 199, question 6