---
title: "DS502- HW2"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    # highlight: zenburn
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1.(5 points) Section 4.7, page 168, question 1

(4.2) : $p(X) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}$

$\frac{p(X)}{1 - p(X)} = \frac{\frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}}{1- \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}}$

$= \frac{\frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}}{\frac{{1 + e^{\beta_0 + \beta_1 x}}}{{1 + e^{\beta_0 + \beta_1 x}}}- \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}}$

$= \frac{\frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}}{\frac{1}{1 + e^{\beta_0 + \beta_1 x}}}$

$=e^{\beta_0 + \beta_1 x}$

(4.3) : $\frac{p(X)}{1 - p(X)}=e^{\beta_0 + \beta_1 x}$

Therefore (4.2) is equivalent to (4.3)

### 2.(10 points) Section 4.7, page 169-170, question 5

a. If the Bayes' decision boundary is linear, QDA is expected to perform better on the train data since it is more flexible and will tend to overfit data. However, on the test data, LDA is expected to perform better since is expected to have lower variance than QDA. Owing to overfitting on the training data, QDA might perform worse on the test data than LDA

b. If the Bayes' decision boundary is non-linear, QDA is expected to perform better on both train and test data since LDA will underfit and wil have high bias.

c. QDA is more flexible than LDA. Therefore it suffers from higher variance. However, variance decreases with more data and the fit will be closer to the actual fit. Therefore, QDA is expected to have better test prediction accuracy with more number of samples.

d. False. Following the previous point, for a linear Bayes' decision boundary, when there are few sample points, QDA will suffer from high variance since it will overfit train data. Therefore, it will have higher test error compared to LDA.

### 3.(5 points) Section 4.7, page 170, question 8

In the case of **logistic regression**, error rate shot up on test data which is an indicator that the model has **high variance**. Therefore the model will tend to perform poor on unseen data. There may be many reasons for this. Perhaps the the decision boundary is not linear.

In the case of **1NN**, since all testing samples would be present in the training data and since the point closest to any point is the point itself, the training error is zero. This makes the testing error as 36% (with avg error rate as 18%).

Although the information provided is not quite sufficient, we should prefer to use logistic model with lower test error rate and less difference between training and test error rate (more generalized)

### 4.(15 points) Section 4.7, page171, question 10

a. 
```{r 4.7 10 a, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
require(ISLR)
data("Weekly")
data.weekly = Weekly
summary(data.weekly)

# standardize data
data.weekly[ , colnames(data.weekly) != "Direction"] = scale(
  data.weekly[ , colnames(data.weekly) != "Direction"])

# pair plot
pairs(data.weekly)

# box plots w.r.t. response variables
par(mfrow=c(3,3))

vars = setdiff(names(Weekly), c("Today", "Direction"))

plot_against_direction <- function(y){
  x = "Direction"
  f <- as.formula(paste(c(y, x), collapse = " ~ "))
  boxplot(f, data= Weekly, ylab = y, xlab = x)
}

sapply(vars, plot_against_direction)
```

+ `Year` is non-linearly related to `Volume`. At first, `Volume` increases rapidly with `Year` but then becomes constant.
+ From the shape of graphs of `Year` against `Lag` variables, the variability in `Lag` vars is higher in the middle and in the end `Year`
+ Similarly, the variablility in `Lag` variables is higher towards either end (high or low) of `Volume` than the middle region.

b. 

```{r 4.7 10 b, echo=TRUE}
logistic.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                    data = data.weekly,
                    family = "binomial")
summary(logistic.fit)
```
From the `summary` of the fit, we see that `Lag2` is a significant factor in predicting `Direction`

c. 

```{r 4.7 10 c, echo=TRUE}
pred <- round(predict(logistic.fit, data.weekly, type = "response"))
pred <- factor(x = pred, labels = c("Down", "Up"))
actual <- data.weekly$Direction
conf_matrix <- table(actual, pred)
message("Confusion matrix")
conf_matrix
message("Fraction of correct predictions:")
(conf_matrix[1] + conf_matrix[4])/sum(conf_matrix)
```

Consider that the logistic model is predicting "is the return positive (`Up`)?"
Now from the confusion matrix the model is making `430` **type 1** errors and `48` **type 2** errors

d. 

```{r 4.7 10 d, echo=TRUE}
train.data <- data.weekly[Weekly$Year <= 2008, ]
heldout.data <- data.weekly[Weekly$Year > 2008, ]

lag2.fit <- glm(Direction ~ Lag2, data = train.data, family = "binomial")

lag2_pred <- round(predict(lag2.fit, heldout.data, type = "response"))
lag2_pred <- factor(x = lag2_pred, labels = c("Down", "Up"))
lag2_actual <- heldout.data$Direction
lag2_conf_matrix <- table(lag2_actual, lag2_pred)
message("Confusion matrix for Lag2 model")
lag2_conf_matrix
message("Fraction of correct predictions:")
(lag2_conf_matrix[1] + lag2_conf_matrix[4])/sum(lag2_conf_matrix)

```

e.

```{r 4.7 10 e, echo=TRUE}
require(MASS)
lda.fit <- lda(Direction ~ Lag2, data = train.data)

lda_pred <- predict(lda.fit, heldout.data)$class
lda_actual <- heldout.data$Direction
lda_conf_matrix <- table(lda_actual, lda_pred)
message("Confusion matrix for lda model")
lda_conf_matrix
message("Fraction of correct predictions:")
(lda_conf_matrix[1] + lda_conf_matrix[4])/sum(lda_conf_matrix)
```

f.

```{r 4.7 10 f, echo=TRUE}

qda.fit <- qda(Direction ~ Lag2, data = train.data)

qda_pred <- predict(qda.fit, heldout.data)$class
qda_actual <- heldout.data$Direction
qda_conf_matrix <- table(qda_actual, qda_pred)
message("Confusion matrix for lda model")
qda_conf_matrix
message("Fraction of correct predictions:")
(qda_conf_matrix[1] + qda_conf_matrix[4])/sum(qda_conf_matrix)

```
g.

```{r 4.7 10 g, echo=TRUE}
require(class)

set.seed(123)

trainX <- as.matrix(train.data$Lag2)
testX <- as.matrix(heldout.data$Lag2)
knn_pred <- knn(trainX, testX, train.data$Direction, k=1)
knn_actual <- heldout.data$Direction
knn_conf_matrix <- table(knn_actual, knn_pred)
message("Confusion matrix for knn=1 model")
knn_conf_matrix
message("Fraction of correct predictions:")
(knn_conf_matrix[1] + knn_conf_matrix[4])/sum(knn_conf_matrix)
```

h. (i)
After experimenting with interaction variables, k and variable transformation, we conclude that Logistic Regression and LDA work best for the `Weekly` dataset.

### 5.(15 points) Section 4.7, page 171-172, question 11

a.
```{r 4.7 11 a, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
data("Auto")
Auto$mpg01 <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto = Auto[, names(Auto) != "mpg"]
Auto$mpg01 <- as.factor(Auto$mpg01)

```

b.
```{r 4.7 11 b, echo=TRUE, fig.height= 8, fig.align='center', results='hide'}
pairs(Auto)

par(mfrow=c(3,3))

vars = setdiff(names(Auto), c("mpg01", "name"))

plot_against_mpg01 <- function(y){
  x = "mpg01"
  f <- as.formula(paste(c(y, x), collapse = " ~ "))
  boxplot(f, data = Auto, ylab = y, xlab = x)
}

sapply(vars, plot_against_mpg01)

hist(Auto$cylinders[Auto$mpg01 == 0 ], xlab = "cylinders (mpg == 0)", main = "")
hist(Auto$cylinders[Auto$mpg01 == 1 ], xlab = "cylinders (mpg == 1)", main = "")
```

From the plots:

  + From the boxplot of `cylinders` vs `mpg01`, `cylinders` is probably the most important predictor of `mpg`. Most of the `1` class are at 4 cylinders.
  + `weight` could be a good predictor of `mpg01` since the boxplot shows that the medians are quite apart and overlap is small. Similarly, `displacement` and `horsepower` could also be good predictors.
  + There is strong correlation between variables like `weight`, `horsepower`, `displacement`. So there's probably redundant information in these variables.
  
c.
```{r 4.7 11 c, echo=TRUE}
perc_train <- 0.8
n_rows <- nrow(Auto)
n_train <- round(perc_train * n_rows)

samples = sample(1:n_rows, n_train, replace=FALSE)
train = logical(n_rows)
train[samples] = TRUE
test = !train

train.data <- Auto[train, ]
test.data <- Auto[test, ]

dim(train.data)
dim(test.data)
```

d.
```{r 4.7 11 d, echo=TRUE}
lda.fit <- lda(mpg01 ~ . -name -year -acceleration, data = train.data)

lda_pred <- predict(lda.fit, test.data)$class
lda_actual <- test.data$mpg01
lda_conf_matrix <- table(lda_actual, lda_pred)
message("Confusion matrix for lda model")
lda_conf_matrix
message("Error rate of LDA:")
(lda_conf_matrix[2] + lda_conf_matrix[3])/sum(lda_conf_matrix)
```

e.
```{r 4.7 11 e, echo=TRUE}
qda.fit <- qda(mpg01 ~ . -name -year -acceleration, data = train.data)

qda_pred <- predict(qda.fit, test.data)$class
qda_actual <- test.data$mpg01
qda_conf_matrix <- table(qda_actual, qda_pred)
message("Confusion matrix for lda model")
qda_conf_matrix
message("Error rate of QDA:")
(qda_conf_matrix[2] + qda_conf_matrix[3])/sum(qda_conf_matrix)
```

f. 
```{r 4.7 11 f, echo=TRUE}
consider.columns = !(names(train.data) %in% c("name", "year", "acceleration"))
logistic.fit <- glm(mpg01 ~ ., data = train.data[, consider.columns], family = "binomial")
summary(logistic.fit)

pred <- round(predict(logistic.fit, test.data[, consider.columns], type = "response"))
pred <- factor(x = pred, labels = c("0", "1"))
actual <- test.data$mpg01
conf_matrix <- table(actual, pred)
message("Confusion matrix")
conf_matrix
message("Error rate for logistic regression:")
(conf_matrix[2] + conf_matrix[3])/sum(conf_matrix)
```

g.
```{r 4.7 11 g, echo=TRUE}
knn_features <- !(names(train.data) %in% c("name", "year", "acceleration", "mpg01"))
trainX <- as.matrix(train.data[, knn_features])
testX <- as.matrix(test.data[, knn_features])

calculate_knn<- function(i) {
  set.seed(123)
  knn_pred <- knn(trainX, testX, train.data$mpg01, k=i)
  knn_actual <- test.data$mpg01
  knn_conf_matrix <- table(knn_actual, knn_pred)
  # sprintf("Confusion matrix for knn=1 model")
  # print(knn_conf_matrix)
  error_rate <- (knn_conf_matrix[2] + knn_conf_matrix[3])/sum(knn_conf_matrix)
  return(c(i, error_rate))
}

k_data <- data.frame(t(sapply(seq(from=1, to=51, by=2), calculate_knn)))
colnames(k_data) <- c("k", "error_rate")
plot(error_rate ~ k, data=k_data)

best_k <- k_data$k[k_data$error_rate == min(k_data$error_rate)]
best_k <- paste(as.vector(best_k), collapse = ", ")

sprintf("From the plot, k=%s seems to perform better on this dataset", best_k)
```


### 6.(10 points) Section 5.4, page 197, question 1

We know that:

$\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2 \mathrm{Cov}(X,Y)$

$\mathrm{Var}(aX) = a^2\mathrm{Var}(X)$

And $\mathrm{Cov}(aX,bY) = ab\mathrm{Cov}(X,Y)$

So, 

$\mathrm{Var}(\alpha X + (1 - \alpha) Y) = \alpha^2 \mathrm{Var}(X) + (1-\alpha)^2 \mathrm{Var}(Y) + 2 \alpha (1-\alpha)\mathrm{Cov}(X,Y)$

$= \sigma_{X}^2 \alpha^2 + \sigma_{Y}^2 (1-\alpha)^2 + 2\sigma_{XY}(\alpha -\alpha^2)$

In order to minimize $\mathrm{Var}(\alpha X + (1 - \alpha) Y)$, we derive this quantity respect to $\alpha$ to find the critical points.
$\frac{\partial}{\partial\alpha}\mathrm{Var}(\alpha X + (1 - \alpha) Y) = 2\alpha\sigma_X^2 - 2\sigma_Y^2 + 2\alpha\sigma_Y^2 + 2\sigma_{XY} - 4\alpha\sigma_{XY}  = 2\alpha (\sigma_X^2+\sigma_Y^2-2\sigma_{XY}) - 2 (\sigma_Y^2-\sigma_{XY})$ 

Hence, the critical points correspond to $\alpha$ where the previous equation is equal to 0.

Meaning,

$\alpha (\sigma_X^2+\sigma_Y^2-2\sigma_{XY}) - (\sigma_Y^2-\sigma_{XY}) = 0$

So,

$\alpha = \frac{\sigma_Y^2-\sigma_{XY}}{\sigma_X^2+\sigma_Y^2-2\sigma_{XY}}$

Now that we know that $\alpha$ is a critical point, we should prove that it corresponds to a minimum. For this, we should consider the second derivative of $\mathrm{Var}(\alpha X + (1 - \alpha) Y)$ and prove that it is positive.

$\frac{\partial^2}{\partial\alpha^2}\mathrm{Var}(\alpha X + (1 - \alpha) Y) = 2\sigma_X^2 + 2\sigma_Y^2 - 4\sigma_{XY} = 2\mathrm{Var}(X - Y)\ge 0$

Then, $\alpha = \frac{\sigma_Y^2-\sigma_{XY}}{\sigma_X^2+\sigma_Y^2-2\sigma_{XY}}$ is a minimum.

### 7.(10 points) Section 5.4, page 197, question 2

a.

Let's consider $p'$ the probabilty that the first bootstrap observation *is* the jth observation from the original sample. Hence, $p' = \frac{1}{n}$.

So the probability that the first bootstrap observation *is not* the jth observation from the original sample $p = 1 - p' = 1 - \frac{1}{n}$

Another way to explain is that the number of elements exculding the jth elements in the original observation is $n - 1$, hence, the probability to pick any item except the jth is $p = \frac{n-1}{n} = 1 - \frac{1}{n}$

b.

Given the fact that we are using a sampling with replacement, during each iteration, the set of observations will be the same as the initial one, that's said, the probability that the second bootstrap observation is not the jth observation from the original sample is the same as above.
$p = \frac{n-1}{n} = 1 - \frac{1}{n}$

c.

We know that the probility that the ith bootstrap observation is not the jth observation from the original sample is $p = \frac{n-1}{n} = 1 - \frac{1}{n}$ and that the total bootstrapping size is $n$.

As we are sampling with replacement, all the probabilities of the observations are independent of one another.

So, the probability that the jth observation is not in the bootstrap sample is $p^n = (1 - \frac{1}{n})^n$

d.

The probability that the jth observation is in the bootstrap sample is $p_d = 1 - p^5 = 1 - (1 - \frac{1}{n})^n$

With $n=5$, $p_d = 1 - (1 - \frac{1}{5})^5 = 0.672$

e.

With $n=100$, $p_d = 1 - (1 - \frac{1}{100})^{100} = 0.634$


f.

With $n=10000$, $p_d = 1 - (1 - \frac{1}{10000})^{10000} = 0.632$

g.

```{r 5.4 2 g}
# Assign to x the integer sequence of range (1,100000)
x=1:100000

# Apply the function of probabilty to the previous sequence
y=sapply(x,function(n){1-((1-(1/n))^n)})

# Display the curve
plot(x,y,xlab="n",ylab="Pd",log="x")
```

The probability seems to reach an asymptote quickly around $0.63$ for around $n=100$ which could be explained by the fact that there is for these cases 63% chance that a given observation will be in the bootstrap sample even for datasets containing a large number of observations.

h.

```{r 5.4 2 h}
#set.seed(211)
store=rep(NA, 10000)
for(i in 1:10000){
  store[i]=sum(sample(1:100, rep=TRUE)==4)>0 
}
mean(store)
```

The algorithm above creates a list of length 10000, then iteratively samples 100 times with replacement checking if the 4th element is in the list. We observe again that the probability is around 0.632 (for seed=211, it is equal to 0.638).

We know that,

$\lim_{n\rightarrow\infty}(1 + x/n)^n = e^x$

Hence, 
$\lim_{n\rightarrow\infty}1-((1-(1/n))^n) = 1 - e^{-1} = 1 - \frac{1}{e} = 0.632$

Which explains the asymptote that the curve follows in the previous question.

### 8.(15 points) Section 5.4, page 198, question 5

a. 
The following code builds a logistic regression model based on all the instances of the dataset Default using `income` and `balance` to predict `default` 

```{r 5.4 5 a}
# Import the library ISLR
library(ISLR)
# Attach Default as the default dataset
attach(Default)
# Fix the random seed
set.seed(5)
# Build a logistic regression model
glmFit = glm(default ~ income + balance, data = Default, family = "binomial")
# Print the model details
summary(glmFit)

```

b. (i)
We begin by splitting the sample set into training and validation sets (80%/20%)

``` {r 5.4 5 b i}
# Get the sample indices
indices = sample(1:nrow(Default),nrow(Default)*0.8)
# Assign the training subset to a variable
trainingSet = Default[indices, ]
# Assign the testing subset to a variable
testingSet = Default[-indices,]
```

(ii)

``` {r 5.4 5 b ii}
# Train a logistic regression model using the training set only
trainFit = glm(default ~ income + balance, data = trainingSet, family = "binomial")
# Display the model details
summary(trainFit)
```

(iii)

``` {r 5.4 5 b iii}
# Predict the default values of the testing set using the built model (with training set)
prediction = predict(trainFit,testingSet,type="response")

# Classify the default category of the default value
class = ifelse(prediction > 0.5,"Yes","No")

# Display the confusion matrix (in a table)
table(testingSet$default, class, dnn=c("Actual","Predicted"))
```

(iv)
``` {r 5.4 5 b iv}
# Compute the validation set error
mean(class!=testingSet$default)
```

c.
We repeat the same process three times by changing the apportionment of data into training and testing sets.
``` {r 5.4 5 c i}
# First sample (70%/30%)
indices = sample(1:nrow(Default),nrow(Default)*0.7)
trainingSet = Default[indices, ]
testingSet = Default[-indices,]
trainFit = glm(default ~ income + balance, data = trainingSet, family = "binomial")
prediction = predict(trainFit,testingSet,type="response")
class = ifelse(prediction > 0.5,"Yes","No")
table(testingSet$default, class, dnn=c("Actual","Predicted"))
mean(class!=testingSet$default)
```

``` {r 5.4 5 c ii}
# Second sample (50%/50%)
indices = sample(1:nrow(Default),nrow(Default)*0.5)
trainingSet = Default[indices, ]
testingSet = Default[-indices,]
trainFit = glm(default ~ income + balance, data = trainingSet, family = "binomial")
prediction = predict(trainFit,testingSet,type="response")
class = ifelse(prediction > 0.5,"Yes","No")
table(testingSet$default, class, dnn=c("Actual","Predicted"))
mean(class!=testingSet$default)
```

``` {r 5.4 5 c iii}
# Third sample (10%/90%)
indices = sample(1:nrow(Default),nrow(Default)*0.1)
trainingSet = Default[indices, ]
testingSet = Default[-indices,]
trainFit = glm(default ~ income + balance, data = trainingSet, family = "binomial")
prediction = predict(trainFit,testingSet,type="response")
class = ifelse(prediction > 0.5,"Yes","No")
table(testingSet$default, class, dnn=c("Actual","Predicted"))
mean(class!=testingSet$default)
```

We can observe that the value of the validation error is different for each sample of data even though it is taken from the same dataset. That indicates that the error is dependent on the composition of the training and testing subsets. However, as illustrated by the last sample, which uses an uncommon ratio (only 10% for training), the error is not linearly dependent on the amount of data used to train the model since the error for the last error is less than the validation error for the 50/50 sample.

d.

In this example, we use the 80/20 ratio for sampling our data (since it has the lowest error) and predict the values of `default` using a logistic regression model based on `income`, `balance` and `student`.

``` {r 5.4 5 d}
# We add the dummy variable student to the logistic regression model
indices = sample(1:nrow(Default),nrow(Default)*0.8)
trainingSet = Default[indices, ]
testingSet = Default[-indices,]
trainFit = glm(default ~ income + balance + student, data = trainingSet, family = "binomial")
prediction = predict(trainFit,testingSet,type="response")
class = ifelse(prediction > 0.5,"Yes","No")
table(testingSet$default, class, dnn=c("Actual","Predicted"))
mean(class!=testingSet$default)
```

We notice that the validation error is reduced after adding the dummy variable `student` to the model.

### 9.(15 points) Section 5.4, page 199, question 6
a.

```{r 5.4 6 a}
# Fix the random seed
set.seed(5)
# Create a logistic regression model based on income and balance
glmFit = glm(default~income+balance,data=Default, family="binomial")
# Display the standard error coefficients
summary(glmFit)$coef[,2]
```

b.

``` {r 5.4 6 b}
# Define a function that takes a dataset and an index as parameters and returns estimates 
# of the income and balance variables
boot.fn = function(data,index){
  fit = glm(default ~ income + balance,data=data,family="binomial",subset=index)
  return(coef(fit))
}
```

c.

```{r 5.4 6 c}
# Import the library boot
library(boot)

# Call the bootstrap function with Default as the data parameter, the function boot.fn as the statistic parameter and 1000 as number of bootstrap replicates
boot(Default, boot.fn, 1000)
```

d.
We notice that the estimated values of the standard error using the bootstrap method are really close from those generated by the `glm()` function. This means that $\hat{\alpha}$ is a good estimator of the real value of $\alpha$.