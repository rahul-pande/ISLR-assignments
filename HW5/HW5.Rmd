---
title: "DS502- HW5"
author: "Mahdi Alouane and Rahul Pande"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      tidy=TRUE,
                      fig.align='center',
                      tidy.opts=list(width.cutoff=60))
# https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html
```

### 1.(10 points) Section 8.4, Page 332, question 3

```{r 8.4 3, echo=TRUE}
library(RColorBrewer)

p = seq(0, 1, 0.01)
gini.index = p * (1 - p) * 2
entropy = -(p * log(p) + (1 - p) * log(1 - p))
binary.err = 1 - pmax(p, 1 - p)
matplot(p, cbind(gini.index, entropy, binary.err),
        xlab = "p", ylab = "value", lwd = 2,
        col = brewer.pal(name = "Set1", n = 3), type = "l", lty = 1)
legend("topleft", c("Gini Index", "Cross Entropy", "Binary Classification error"),
       col = brewer.pal(name = "Set1", n = 3), lty = 1, cex = 0.5)
```


### 2.(15 points) Section 8.4, Page 334, question 9


a)
```{r 8.4 9 a, echo=TRUE}
library(ISLR)
attach(OJ)
set.seed(123)
n = nrow(OJ)
train.data = sample(1:n, 800)
X.test = OJ[-train.data, ]
y.test = OJ$Purchase[-train.data]
```

b)
```{r 8.4 9 b, echo=TRUE}
library(tree)
tree.fit = tree(Purchase~., data = OJ, subset = train.data)
summary(tree.fit)
```

From the summary, we see that the training error is 16.12%. Also, we see that there are 10 terminal nodes in the tree, and the variables used in forming the tree are "LoyalCH", "PriceDiff", "SpecialCH", "PctDiscMM".


c)
```{r 8.4 9 c, echo=TRUE}
tree.fit
levels(OJ$Purchase)
```
Let's take `24) PriceDiff < -0.165 40   47.050 MM ( 0.27500 0.72500 ) *` terminal node. This denotes that for this node  `PriceDiff` is the splitting variable and if `PriceDiff` < -0.165 then the class is MM. Also the number 40 denotes the number of samples in this region (i.e. `PriceDiff` < -0.165). The numbers in the round braces denote the probabilities of the sample being in a class. The order of probabilities is the same as the order of the levels of the response variable. Since - `Levels: CH MM` therefore the probability that sample belongs to `MM` `Purchase` is 0.72500


d)
```{r 8.4 9 d, echo=TRUE}
plot(tree.fit)
text(tree.fit, pretty = 0)
```

+ From the plot we can see that the depth of the tree is four levels
+ The root node splitting variable is `LoyalCH`, i.e. `LoyalCH` is the most important splitting variable.
+ Also, `LoyalCH` is present in first three levels of tree. The first two levels are entirely composed of `LoyalCH` which clearly establishes that is a very important variable.
+ There are 10 terminal nodes where, four of which predict `MM` `Purchase` while other six predict `CH` `Purchase`


e)
```{r 8.4 9 e, echo=TRUE}
y.pred = predict(tree.fit, X.test, type = "class")
conf.mat <- table(y.test, y.pred)

conf.mat

error.rate <- 1 - (conf.mat[1] + conf.mat[4]) / sum(conf.mat)
error.rate
```

The error rate is 17.78%

f)
```{r 8.4 9 f, echo=TRUE}
set.seed(123)
cv.fit = cv.tree(tree.fit, FUN = prune.misclass)
cv.fit
```

g)
```{r 8.4 9 g, echo=TRUE}
plot(cv.fit$size, cv.fit$dev, type = "b", xlab = "Tree Size", ylab = "Classification Errors")
```

h)
```{r 8.4 9 h, echo=TRUE}
```
We see that tree size of 5 corresponds to the lowest cv classification errors.


```{r 8.4 9 i, echo=TRUE}
pruned.fit = prune.tree(tree.fit, best = 5)
```

```{r 8.4 9 j, echo=TRUE}
summary(tree.fit)
summary(pruned.fit)
```

We see that the pruned tree has a higher training classification error (19.12%) than the unpruned tree (16.12%), however has better cross validation error.

```{r 8.4 9 k, echo=TRUE}
y.pred.pruned = predict(pruned.fit, X.test, type = "class")
conf.mat.pruned <- table(y.test, y.pred.pruned)

conf.mat.pruned

error.rate.pruned <- 1 - (conf.mat.pruned[1] + conf.mat.pruned[4]) / sum(conf.mat.pruned)
error.rate.pruned
```

The test error of pruned tree (18.89%) is slighly higher than the unpruned tree (17.78%), however the pruned tree has much lower variance given it has only 5 nodes (half the unpruned tree) as hence has more generalization and more hope of performing better on unseen data.


### 3.(10 points) Section 9.7, Page 368, question 2

a.

The equation provided in the question corresponds to a circle of radius $\sqrt{4} =2$ and of center (-1,2). The sketch of the non-linear decision boundary (circle) is defined below:

```{r 9.7 2 a}
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE,)
```

b.

Inside the circle (<4) and on its boundary (=4), the set of points respects the following equation $(1+X_1)^2 + (2-X_2)^2 \le 4$ (represented in red), however, outside of the circle, the set of points respects the following equation $(1+X_1)^2 + (2-X_2)^2 > 4$ (represented in blue)

```{r 9.7 2 b}
plot(NA, NA, type = "n", xlim = c(-4, 2), ylim = c(-1, 5), asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), rectangles = matrix(c(14,7),nrow=1), add = TRUE, inches = FALSE, bg="skyblue")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE, bg="firebrick2", fg="firebrick2")
text(c(-1), c(2), "<= 4")
text(c(-4), c(2), "> 4")
```

c.

We replace the values of $X_1$ and $X_2$ in the equation above for each of these points as shown below:

| X1 | X2 | Eq     | Class |
|----|----|--------|-------|
| 0  | 0  | 5 > 4  | Blue  |
| -1 | 1  | 1 < 4  | Red   |
| 2  | 2  | 9 > 4  | Blue  |
| 3  | 8  | 52 > 4 | Blue  |



```{r 9.7 2 c}
plot(c(0, -1, 2, 3), c(0, 1, 2, 8), col = c("blue", "red", "blue", "blue"), 
    type = "p", asp = 1, xlab = "X1", ylab = "X2")
symbols(c(-1), c(2), circles = c(2), add = TRUE, inches = FALSE)
text(c(1), c(0), "(0,0)")
text(c(-1), c(1.5), "(-1,1)")
text(c(2), c(1.5), "(2,2)")
text(c(3), c(7.5), "(3,8)")
```

d.

Let's first expand the equation of the circle (decision boundary) given in the question.

$(1+X_1)^2 + (2-X_2)^2 = 4$

$\Leftrightarrow X_1^2 + X_2^2 + 2X_1 - 4X_2 + 1 = 0$

It is clear that this equation is not linear in $X_1$ and $X_2$ since it includes terms in $X_1^2$ and $X_2^2$, however, this equation is linear in terms of $X_1$, $X_1^2$, $X_2$ and $X_2^2$.


### 4.(20 points) Section 9.7, Page 369, question 4 (Open ended question)

```{r 9.7 4 setup, echo=TRUE}
set.seed(123)
X = matrix(rnorm(100*2, sd = 0.5), ncol = 2)
X[1:50, 1] <- X[1:50, 1] + 2
X[51:75, 2] <- X[51:75, 2] + 2
X[1:25, 2] <- X[1:25, 2] + 2
y = c(rep(-1, 25), rep(1, 50), rep(-1, 25))

plot(X, col = y + 2, xlab = "X1", ylab = "X2")
```


```{r 9.7 4 linear, echo=TRUE}
set.seed(123)
train = sample(1:100, 60)
data = data.frame(X = X, y = as.factor(y))

library(e1071)
linear.svm.fit = svm(y~., data = data[train, ], kernel = "linear")
plot(linear.svm.fit, data = data[train, ])

linear.yhat = predict(linear.svm.fit)
linear.pred = predict(linear.svm.fit, newdata = data[-train, ])

# training error
mean(data[train, "y"] != linear.yhat)
# testing error
mean(data[-train, "y"] != linear.pred)
```

SVC is Support Vector Classifier, i.e. Linear SVM. We see that the data is not separated by SVC boundary since data is not linearly separable in the given two dimensions. We see high train and test error of 43% and 60% respectively. So this model is worse than coin flip in case of test.

```{r 9.7 4 poly, echo=TRUE}
poly.svm.fit = svm(y~., data = data[train, ], kernel = "polynomial", degree= 2)
plot(poly.svm.fit, data = data[train, ])

poly.yhat = predict(poly.svm.fit)
poly.pred = predict(poly.svm.fit, newdata = data[-train, ])

# training error
mean(data[train, "y"] != poly.yhat)
# testing error
mean(data[-train, "y"] != poly.pred)
```

Polynomial SVM performs very good on this data with 3.33% train and 2.5% test error respectively. We can see that the polynomial decision boundary separates the data very well.

```{r 9.7 4 rbf-tune, echo=TRUE}
best.fit <- tune(svm, y~., data = data[train, ], kernel = "radial",
                ranges = list(
                  cost = c(0.1,1,10,100,1000),
                  gamma = c(0.5,1,2,3,4)
                  )
                )
summary(best.fit)
```


```{r 9.7 4 rbf, echo=TRUE}
rbf.svm.fit = svm(y~., data = data[train, ], kernel = "radial",
                  gamma = best.fit$best.parameters$gamma, cost = best.fit$best.parameters$cost)
plot(rbf.svm.fit, data = data)

rbf.yhat = predict(rbf.svm.fit)
rbf.pred = predict(rbf.svm.fit, newdata = data[-train, ])

# training error
mean(data[train, "y"] != rbf.yhat)
# testing error
mean(data[-train, "y"] != rbf.pred)
```

We see that RBF SVM also performs very good on this data. The boundary separates the data well. Training error is higher as compared to Polynomial SVM however test error is the same.

For the current data both Polynomial SVM (degree 2) and RBF SVM perform equally better

### 5. (15 points) Section 9.7, Page 369-370, question 5

a.

We generate 500 data points centered around 0 within a 2-dimensional space and belonging to two different calsses separated by a quadratic decision boundary.

```{r 9.7 5 a}
set.seed(10)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1 * (x1^2 - x2^2 > 0)
```

b.

We plot the data generated in a.

```{r 9.7 5 b}
plot(x1, x2, xlab = "X1", ylab = "X2", col = (3 - y), pch = (2 - y))
```

c.

We fit this data with a logistic regression model as shown below:

```{r 9.7 5 c}
logit.fit <- glm(y ~ x1 + x2, family = "binomial")
summary(logit.fit)
```

We can observe that none of the features is statistically significant.

d.

```{r 9.7 5 d}
data <- data.frame(x1 = x1, x2 = x2, y = y)
probs <- predict(logit.fit, data, type = "response")
preds <- rep(0, 500)
preds[probs > 0.47] <- 1
plot(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = (3 - 1), pch = (2 - 1), xlab = "X1", ylab = "X2")
points(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = (3 - 0), pch = (2 - 0))
```

We can observe that the decision boundary is clearly linear as expected.

e.

Now, we fit our model with a non-linear logistic regression model involving the quadratic terms for $X_1$ and $X_2$.

```{r 9.7 5 e}
logitnl.fit <- glm(y ~ poly(x1, 2) + poly(x2, 2), family = "binomial")
```

f.

We plot the predicted classes according to our new model.

```{r 9.7 5 f}
probs <- predict(logitnl.fit, data, type = "response")
preds <- rep(0, 500)
preds[probs > 0.5] <- 1
plot(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = (3 - 1), pch = (2 - 1), xlab = "X1", ylab = "X2")
points(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = (3 - 0), pch = (2 - 0))
```

We can observe that the logistic regression model now has non-linear decision boundary that looks like our initial dataset.

g.

We fit an SVM model to our data with a high budget (small penalty).

```{r 9.7 5 g}
library(e1071)
data$y <- as.factor(data$y)
# Fit SVM with a small penalty
svm.fit <- svm(y ~ x1 + x2, data, kernel = "linear", cost = 0.01)
# Predict and summarize the predictions
preds <- predict(svm.fit, data)
summary(preds)
# Plot the data
plot(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = (3 - 0), pch = (2 - 0), xlab = "X1", ylab = "X2")
points(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = (3 - 1), pch = (2 - 1))
```

We can observe that the decision boundary is linear for this model since we use a linear kernel and that almost all the data got classified in the same class (the red class below the line has 423 data points) even though we assigned a high budget to SVM because it tries to optimally separate a non-linearly seperated data with a line, which might occur to a good prediction to class 0 but a poor prediction in class 1 (a lot of data points predicted as 1 are actually 0)

h.
```{r 9.7 5 h}
data$y <- as.factor(data$y)
svmnl.fit <- svm(y ~ x1 + x2, data, kernel = "radial")
preds <- predict(svmnl.fit, data)
plot(data[preds == 0, ]$x1, data[preds == 0, ]$x2, col = (3 - 0), pch = (2 - 0), xlab = "X1", ylab = "X2")
points(data[preds == 1, ]$x1, data[preds == 1, ]$x2, col = (3 - 1), pch = (2 - 1))
```

We can observe that now that we changed the linear kernel to radial, the decision boundary is non-linear again and it looks similar to our initial data separation.

(i).

From the above experiments, we may conclude that both SVM with a linear kernal and logistic regression (without quadratic terms) perform poorly on non-linear decision boundaries, however, both SVM with a radial kernel and adding quadratic terms (non linear in $X_s$ terms) to the logistic regression model perform really well on non-linear decision boundaries.

### 6. (10 points) Section 10.7, Page 414-415, question 4

a.

There is not enough information to tell. 
For example, let's assume that d(1,4)=4, d(1,5)=1, d(2,4)=2, d(2,5)=3, d(3,4)=4 and d(3,5)=7, the single linkage dissimilarity between {1,2,3} and {4,5} would be equal to 1 and the complete linkage dissimilarity between {1,2,3} and {4,5} would be equal to 7.
That said, with single linkage, they would fuse at a height of 1, and with complete linkage, they would fuse at a height of 7. 
But, if all inter-observations distance are equal to 3, we would have that the single and complete linkage dissimilarities between {1,2,3} and {4,5} are equal to 3.

b.

They would fuse at the same height. 
Assuming that d(5,6)=d, the single and complete linkage dissimilarities between {5} and {6} would be equal to d. Hence, they would fuse at the same height (equal to d) for single and complete linkage.

### 7. (20 points) Section 10.7, Page 417, question 10

a.

We generate a simulated set of data containing 60 observations divided equally into 3 distinct classes (20 data points per class). We added a mean shift of 0.7 between the three classes to seperate them visually.

```{r 9.7 7 a}
set.seed(11)
X <- rbind(matrix(rnorm(20*50, mean = 0), nrow = 20),
matrix(rnorm(20*50, mean=0.7), nrow = 20),
matrix(rnorm(20*50, mean=1.4), nrow = 20))
```

b.

We perform PCA on the 60 observations generated above and then we plot them according to the first two principal components PC1 and PC2 as shown below:

```{r 9.7 7 b}
# Perform PCA
X.pca = prcomp(X)$x
# Plot according to the first two principal components
plot(X.pca[,1:2], col=c(rep(1,20), rep(2,20), rep(3,20)))
```

We can see that the three classes are well sperated and easily distinguishable.

c.

```{r 9.7 7 c}
res = kmeans(X, centers = 3)
true_class = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

We can see that the data is perfectly seperated into 3 distincs clusters after performing the 3-mean algorithm since each intial class is now separated in a single cluster (no overlapping).

d.

```{r 9.7 7 d}
res = kmeans(X, centers = 2)
true = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

By executing K-means with $K=2$, we force our algorithm to come out with only 2 clusters, hence, it is obliged to group data points from different classes into the same cluster.
According to the above, 2-means succeeds to separate on class from the two others (class 3 is perfectly clustered), however, the class 1 and 2 are combined in the same cluster.

e.

```{r 9.7 7 e}
res = kmeans(X, centers = 4)
true = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

We call K-means with $K=4$ which requires our algorithm to output 4 different clusters. We can observe that two classes have been well separated (1 and 2 even though 2 instances of class 2 are in cluster 3), however, the other class (class 3) has been divided in two different clusters (cluster 2 and 3) and we can easily obtain it by the union of these clusters.

f.

```{r 9.7 7 f}
res = kmeans(X.pca[,1:2], centers = 3)
true = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

Applying K-means (with $K=3$) on the two principal components after performing PCA generates the same results as in b.

Since PCA preserves pairwise distance between data points and only tries to minimize the variance, we can say that the 2 principal components provide enough information on the data and doesn't affect the output of 3-means.

g.

```{r 9.7 7 g}
res = kmeans(scale(X), centers = 3)
true = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

Since scaling the data also preserves pairwise distance between data points, we can see also that after scaling the data, the output of 3-means is not different from b.
